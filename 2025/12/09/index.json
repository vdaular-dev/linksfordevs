[
  {
    "Title": "Horses",
    "Url": "https://andyljones.com/posts/horses.html",
    "Timestamp": "2025-12-09T06:02:10",
    "Domain": "andyljones.com",
    "Description": "AI progress is steady. Human equivalence is sudden."
  },
  {
    "Title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks",
    "Url": "https://arxiv.org/abs/2512.03262",
    "Timestamp": "2025-12-09T03:02:20",
    "Domain": "arxiv.org",
    "Description": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications."
  },
  {
    "Title": "Hyperacute Interdynamics",
    "Url": "https://xkcd.com/3178/",
    "Timestamp": "2025-12-09T03:02:17",
    "Domain": "xkcd.com",
    "Description": ""
  },
  {
    "Title": "Icons in Menus Everywhere — Send Help",
    "Url": "https://blog.jim-nielsen.com/2025/icons-in-menus/",
    "Timestamp": "2025-12-09T03:02:14",
    "Domain": "blog.jim-nielsen.com",
    "Description": "Writing about the big beautiful mess that is making things for the world wide web."
  },
  {
    "Title": "All that matters is winning",
    "Url": "https://www.ryanhoover.me/post/all-that-matters-is-winning",
    "Timestamp": "2025-12-09T01:04:31",
    "Domain": "www.ryanhoover.me",
    "Description": ""
  },
  {
    "Title": "PostgreSQL and MongoDB: What Scaling Really Looks Like",
    "Url": "https://stormatics.tech/blogs/postgresql-mongodb-and-what-cannot-scale-really-means",
    "Timestamp": "2025-12-09T01:04:27",
    "Domain": "stormatics.tech",
    "Description": "How PostgreSQL scales in real-world systems, compared with MongoDB, and why true scalability depends on design and workloads, not headlines."
  },
  {
    "Title": "i read more than i write",
    "Url": "https://enombic.com/read-more-than-write",
    "Timestamp": "2025-12-09T01:04:18",
    "Domain": "enombic.com",
    "Description": "we all consume. few create. ai trains on the minority."
  },
  {
    "Title": "They have to be able to talk about us without us - Anil Dash",
    "Url": "https://www.anildash.com/2025/12/05/talk-about-us-without-us/",
    "Timestamp": "2025-12-09T01:04:15",
    "Domain": "www.anildash.com",
    "Description": "A blog about making culture. Since 1999."
  },
  {
    "Title": "Satisficing for LLMs",
    "Url": "https://www.alephic.com/writing/satisficing-for-llms",
    "Timestamp": "2025-12-09T01:04:11",
    "Domain": "www.alephic.com",
    "Description": "By applying Herbert Simon’s concept of satisficing to AI, this post argues that language models might prefer logical‐sounding content over emotional appeals, mirroring human biases but inverted. It unveils a paradox: humans use emotion to decide rationally, while LLMs use pseudo‐rational style to appear helpful."
  },
  {
    "Title": "The secure open source fallacy",
    "Url": "https://ulveon.net/p/2025-12-03-the-secure-open-source-fallacy/",
    "Timestamp": "2025-12-09T01:04:07",
    "Domain": "ulveon.net",
    "Description": "Most open source advocates, and many security professionals, often say things like “open source software is secure because you can just read the code”.\nThis argument assumes that the ability to read source code directly translates into the ability to understand, verify, and trust it, because you can see the files this software opens or the network sockets it listens on. You can see the kind of network data it sends, and the cryptography it uses.\n"
  },
  {
    "Title": "Jepsen: NATS 2.12.1",
    "Url": "https://jepsen.io/analyses/nats-2.12.1",
    "Timestamp": "2025-12-09T01:04:03",
    "Domain": "jepsen.io",
    "Description": "NATS is a distributed streaming system. Regular NATS streams offer only best-effort delivery, but a subsystem, called JetStream, guarantees messages are delivered at least once. We tested NATS JetStream, version 2.12.1, and found that it lost writes if data files were truncated or corrupted on a minority of nodes. We also found that coordinated power failures, or an OS crash on a single node combined with network delays or process pauses, can cause the loss of committed writes and persistent split-brain. This data loss was caused (at least in part) by choosing to flush writes to disk every two minutes, rather than before acknowledging them. We also include a belated note on data loss due to process crashes in version 2.10.22, which was fixed in 2.10.23. NATS has now documented the risk of its default fsync policy, and the remaining issues remain under investigation. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the Jepsen ethics policy."
  },
  {
    "Title": "Claude Diary",
    "Url": "https://rlancemartin.github.io/2025/12/01/claude_diary/",
    "Timestamp": "2025-12-09T01:03:59",
    "Domain": "rlancemartin.github.io",
    "Description": "Creating a memory system for Claude Code."
  },
  {
    "Title": "John Ocen's",
    "Url": "https://johnocens.com/soothfare/WeSolvedScalebutLostCohesion",
    "Timestamp": "2025-12-09T01:03:56",
    "Domain": "johnocens.com",
    "Description": "Output is deliberate."
  },
  {
    "Title": "History - 2100",
    "Url": "https://lucasvance.github.io/2100/history/",
    "Timestamp": "2025-12-09T01:03:52",
    "Domain": "lucasvance.github.io",
    "Description": "2100: Restoring the American Dream"
  },
  {
    "Title": "Multibase CLI",
    "Url": "http://www.chriswarbo.net/blog/2025-12-07-multibase_cli.html",
    "Timestamp": "2025-12-09T01:03:48",
    "Domain": "www.chriswarbo.net",
    "Description": "I recently pushed a few patches to the multiformats/rust-multibase\nproject to scratch my own itch, and I thought they would be useful to\ntalk about. In short, these changes do the following:"
  },
  {
    "Title": "NotARoomba",
    "Url": "https://notaroomba.dev/simpleflightcontroller",
    "Timestamp": "2025-12-09T01:03:42",
    "Domain": "notaroomba.dev",
    "Description": ""
  },
  {
    "Title": "Salt Bank Wants You to Gamble",
    "Url": "https://michael-dev-tech.github.io/Website/salt.html",
    "Timestamp": "2025-12-09T01:03:38",
    "Domain": "michael-dev-tech.github.io",
    "Description": "← Back to Home"
  },
  {
    "Title": "llm weights vs the papercuts of corporate",
    "Url": "https://ghuntley.com/papercuts/",
    "Timestamp": "2025-12-09T01:03:35",
    "Domain": "ghuntley.com",
    "Description": "In woodworking, there's a saying that you should work with the grain, not against the grain and I've been thinking about how this concept may apply to large language models.\n\nThese large language models are built by training on existing data. This data forms the backbone which creates output based"
  },
  {
    "Title": "Kinesis Advantage2 — Danish Prakash",
    "Url": "https://danishpraka.sh/posts/kinesis-advantage2/",
    "Timestamp": "2025-12-09T01:03:31",
    "Domain": "danishpraka.sh",
    "Description": ""
  },
  {
    "Title": "Deep Dive into NVIDIA's Virtuous Cycle",
    "Url": "https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle",
    "Timestamp": "2025-12-09T01:03:28",
    "Domain": "philippeoger.com",
    "Description": "I’ve spent the last 48 hours completely falling down the rabbit hole of\nNVIDIA’s Q3 Fiscal 2026 earnings report. If\nyou just skim the headlines, everything looks perfect: Revenue is up 62% to $57\nbillion, and Jensen Huang is talking about a \"virtuous cycle of AI.\""
  },
  {
    "Title": "Going direct: notes on Eli Lilly at a trillion",
    "Url": "https://atelfo.github.io/2025/12/07/going-direct.html",
    "Timestamp": "2025-12-09T01:03:24",
    "Domain": "atelfo.github.io",
    "Description": "Personal site for posts about my interests: the biotech industry, medicine, molecular biology, neuroscience, biorisk, science, consciousness, AI, innovation, decision making, philosophy, games, sci-fi, probability, and forecasting (among other things). I write to learn, mostly about biotech."
  },
  {
    "Title": "Words That Make Language Models Perceive",
    "Url": "https://www.sophielwang.com/sensory",
    "Timestamp": "2025-12-09T01:02:46",
    "Domain": "www.sophielwang.com",
    "Description": "A simple cue like asking the model to 'see' or 'hear' can push a purely text-trained language model towards the representations of purely image-trained or purely-audio trained encoders."
  }
]